From 18b8ae0dfcbf1fe3b0ddc806f7c1df1617b7c98d Mon Sep 17 00:00:00 2001
From: Lin Xie <lin.xie@intel.com>
Date: Mon, 7 Jan 2019 18:02:30 +0800
Subject: [PATCH] Add inference engine C API.

---
 inference-engine/include/ie_api_wrapper.h          | 146 ++++
 inference-engine/include/ie_common_wrapper.h       | 195 +++++
 inference-engine/src/CMakeLists.txt                |   1 +
 inference-engine/src/ie_c_wrapper/CMakeLists.txt   |  29 +
 inference-engine/src/ie_c_wrapper/README.md        |   1 +
 .../src/ie_c_wrapper/ie_api_wrapper.cpp            | 132 ++++
 inference-engine/src/ie_c_wrapper/ie_context.cpp   | 834 +++++++++++++++++++++
 inference-engine/src/ie_c_wrapper/ie_context.h     | 265 +++++++
 8 files changed, 1603 insertions(+)
 create mode 100644 inference-engine/include/ie_api_wrapper.h
 create mode 100644 inference-engine/include/ie_common_wrapper.h
 create mode 100644 inference-engine/src/ie_c_wrapper/CMakeLists.txt
 create mode 100644 inference-engine/src/ie_c_wrapper/README.md
 create mode 100644 inference-engine/src/ie_c_wrapper/ie_api_wrapper.cpp
 create mode 100644 inference-engine/src/ie_c_wrapper/ie_context.cpp
 create mode 100644 inference-engine/src/ie_c_wrapper/ie_context.h

diff --git a/inference-engine/include/ie_api_wrapper.h b/inference-engine/include/ie_api_wrapper.h
new file mode 100644
index 0000000..65293b2
--- /dev/null
+++ b/inference-engine/include/ie_api_wrapper.h
@@ -0,0 +1,146 @@
+ // Licensed under the Apache License, Version 2.0 (the "License");
+ // you may not use this file except in compliance with the License.
+ // You may obtain a copy of the License at
+ //
+ //      http://www.apache.org/licenses/LICENSE-2.0
+ //
+ // Unless required by applicable law or agreed to in writing, software
+ // distributed under the License is distributed on an "AS IS" BASIS,
+ // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ // See the License for the specific language governing permissions and
+ // limitations under the License.
+
+#ifndef _IE_PLUGIN_WRAPPER_H_
+#define _IE_PLUGIN_WRAPPER_H_
+
+#include "ie_common_wrapper.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/*
+ * the API to allocate the Inference engine context
+ * Input : the IEConfig as input
+ * return: the void pointer to the IE context
+ */
+void * IEAllocateContextWithConfig(IEConfig * config);
+
+/*
+* the API to allocate the Inference engine context defaultly
+* return : the void pointer to the IE context
+*/
+void * IEAllocateContext(void);
+
+/*
+ * the API to release the inferencce engine context
+ * input: the pointer o inference engine context
+ */
+void IEFreeContext(void * contextPtr);
+
+/*
+* the API to load the model from IR format
+* input: the pointer o inference engine context
+* Input : the IEConfig as input with IR file name(absolute path)
+* return: the void pointer to the IE context
+*/
+void IELoadModel(void * contextPtr, IEConfig * config);
+
+/*
+* the API to create the model/network and deploy it on the target device.
+* this need to fill in the input info and output info before this API.
+* input: the pointer o inference engine context
+* Input : the IEConfig as input with IR file name(absolute path)
+* return: the void pointer to the IE context
+*/
+void IECreateModel(void * contextPtr, IEConfig * config);
+
+/*
+ * the API to get the size of inference engine context
+ * return the size of context
+ */
+int IESizeOfContext(void);
+
+/*
+* the API to get the size of Input Image
+* input: the pointer o inference engine context
+* return:the Image size structure
+*/
+void IEGetModelInputImageSize(void * contextPtr, IEImageSize * imageSize);
+
+/*
+* the API to get the info of model Input
+* input: the pointer o inference engine context
+* return:the input info of the model
+*/
+void IEGetModelInputInfo(void * contextPtr, IEInputOutputInfo * info);
+
+/*
+* the API to set the info of model Input
+* input: the pointer o inference engine context
+* return:the input info of the model
+*/
+void IESetModelInputInfo(void * contextPtr, IEInputOutputInfo * info);
+
+/*
+* the API to get the info of model output
+* input: the pointer o inference engine context
+* return:the output info of the model
+*/
+void IEGetModelOutputInfo(void * contextPtr, IEInputOutputInfo * info);
+
+/*
+* the API to set the info of model output
+* input: the pointer o inference engine context
+* return:the output info of the model
+*/
+void IESetModelOutputInfo(void * contextPtr, IEInputOutputInfo * info);
+
+/*
+* the API to execute the model in the sync/async mode. Call the IESetInput firstly.
+* input: the pointer o inference engine context
+* input: the sync or async mode. 1: async mode; 0: sync mode. default is 0
+*/
+void IEForward(void * contextPtr, IEInferMode mode);
+
+/*
+* the API to feed the input image to the model
+* input: the pointer o inference engine context
+* input: the index of input, idx can be get from the IEGetModelInputInfo()
+* input: the image to be processed
+*/
+void IESetInput(void * contextPtr, unsigned int idx, IEData * data);
+
+/*
+* the API to set the output buffer of the model
+* input: the pointer o inference engine context
+* input: the output buffer
+*/
+//void IESetOutput(void * contextPtr, IEData * data);
+
+/*
+* the API to get the result pointer after the execution
+* input: the pointer o inference engine context
+* input: the index of output, idx can be get from the IEGetModelOutputInfo()
+* output: the size of result
+*/
+void * IEGetResultSpace(void * contextPtr, unsigned int idx, unsigned int * size);
+
+/*
+* the API to print the log info after the execution
+* input: the pointer o inference engine context
+*/
+void IEPrintLog(void * contextPtr, unsigned int flag);
+
+/*
+* the API to set batch size
+* input: the batch size value to be set
+*/
+void IESetBatchSize(void *contextPtr, int size);
+
+void IESetCpuThreadsNum(void *contextPtr, unsigned int num);
+
+#ifdef __cplusplus
+}
+#endif
+#endif
diff --git a/inference-engine/include/ie_common_wrapper.h b/inference-engine/include/ie_common_wrapper.h
new file mode 100644
index 0000000..2acbbc1
--- /dev/null
+++ b/inference-engine/include/ie_common_wrapper.h
@@ -0,0 +1,195 @@
+ // Licensed under the Apache License, Version 2.0 (the "License");
+ // you may not use this file except in compliance with the License.
+ // You may obtain a copy of the License at
+ //
+ //      http://www.apache.org/licenses/LICENSE-2.0
+ //
+ // Unless required by applicable law or agreed to in writing, software
+ // distributed under the License is distributed on an "AS IS" BASIS,
+ // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ // See the License for the specific language governing permissions and
+ // limitations under the License.
+#ifndef _IE_COMMON_WRAPPER_H_
+#define _IE_COMMON_WRAPPER_H_
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+ /**
+ * @enum TargetDevice
+ * @brief Describes known device types
+ */
+typedef enum tagIETargetDeviceType {
+    IE_Default = 0,
+    IE_Balanced = 1,
+    IE_CPU = 2,
+    IE_GPU = 3,
+    IE_FPGA = 4,
+    IE_MYRIAD = 5,
+    IE_HDDL = 6,
+    IE_GNA = 7,
+    IE_HETERO = 8
+}IETargetDeviceType;
+
+/**
+* @enum Precision
+* @brief Describes Precision types
+*/
+typedef enum tagIEPrecisinType {
+    IE_UNSPECIFIED = 255, /**< Unspecified value. Used by default */
+    IE_MIXED = 0,  /**< Mixed value. Can be received from network. No applicable for tensors */
+    IE_FP32 = 10,  /**< 32bit floating point value */
+    IE_FP16 = 11,  /**< 16bit floating point value */
+    IE_Q78 = 20,   /**< 16bit specific signed fixed point precision */
+    IE_I16 = 30,   /**< 16bit signed integer value */
+    IE_U8 = 40,    /**< 8bit unsigned integer value */
+    IE_I8 = 50,    /**< 8bit signed integer value */
+    IE_U16 = 60,   /**< 16bit unsigned integer value */
+    IE_I32 = 70,   /**< 32bit signed integer value */
+    IE_CUSTOM = 80 /**< custom precision has it's own name and size of elements */
+}IEPrecisionType;
+
+/**
+* @enum Layout
+* @brief Layouts that the inference engine supports
+*/
+typedef enum tagIELayoutType {
+    IE_ANY = 0,// "any" layout
+    // I/O data layouts
+    IE_NCHW = 1,
+    IE_NHWC = 2,
+    // weight layouts
+    IE_OIHW = 64,
+    // bias layouts
+    IE_C = 96,
+    // Single image layout (for mean image)
+    IE_CHW = 128,
+    // 2D
+    IE_HW = 192,
+    IE_NC = 193,
+    IE_CN = 194,
+    IE_BLOCKED = 200,
+}IELayoutType;
+
+/**
+* @enum Memory Type
+* @brief memory type that the inference engine supports?
+*/
+typedef enum tagIEMemoryType {
+    IE_DEVICE_DEFAULT = 0,
+    IE_DEVICE_HOST = 1,
+    IE_DEVICE_GPU = 2,
+    IE_DEVICE_MYRIAD = 3,
+    IE_DEVICE_SHARED = 4,
+}IEMemoryType;
+
+/**
+* @enum Image Channel order Type
+* @brief Image Channel order now the BGR is used in the most model.
+*/
+typedef enum tagIEImageFormatType {
+    IE_IMAGE_FORMAT_UNKNOWN = -1,
+    IE_IMAGE_BGR_PACKED,
+    IE_IMAGE_BGR_PLANAR,
+    IE_IMAGE_RGB_PACKED,
+    IE_IMAGE_RGB_PLANAR,
+    IE_IMAGE_GRAY_PLANAR,
+    IE_IMAGE_GENERIC_1D,
+    IE_IMAGE_GENERIC_2D,
+}IEImageFormatType;
+
+/**
+* @enum IE forward mode: sync/async
+* @brief IE forward mode: sync/async
+*/
+typedef enum tagIEInferMode {
+    IE_INFER_MODE_SYNC = 0,
+    IE_INFER_MODE_ASYNC = 1,
+}IEInferMode;
+
+/**
+* @enum IE data mode: image/non-image
+* @brief IE data mode: image/non-image
+*/
+typedef enum tagIEDataType {
+    IE_DATA_TYPE_NON_IMG = 0,
+    IE_DATA_TYPE_IMG = 1,
+}IEDataType;
+
+/**
+* @enum IE log level
+* @brief IE log level
+*/
+typedef enum tagIELogLevel {
+    IE_LOG_LEVEL_NONE = 0x0,
+    IE_LOG_LEVEL_ENGINE = 0x1,
+    IE_LOG_LEVEL_LAYER = 0x2,
+}IELogLevel;
+
+/**
+* @enum Image Size Type
+* @brief Image Size order now the BGR is used in the most model.
+*/
+typedef struct tagIEImageSize {
+    unsigned int imageWidth;
+    unsigned int imageHeight;
+}IEImageSize;
+
+/**
+* @struct model input info
+* @brief model input info
+*/
+#define IE_MAX_INPUT_OUTPUT 10
+typedef struct tagIEInputOutputInfo {
+    unsigned int width[IE_MAX_INPUT_OUTPUT];
+    unsigned int height[IE_MAX_INPUT_OUTPUT];
+    unsigned int channels[IE_MAX_INPUT_OUTPUT];
+    IEPrecisionType precision[IE_MAX_INPUT_OUTPUT];
+    IELayoutType layout[IE_MAX_INPUT_OUTPUT];
+    IEDataType dataType[IE_MAX_INPUT_OUTPUT];
+    unsigned int batch_size;
+    unsigned int numbers;
+}IEInputOutputInfo;
+
+/**
+* @struct inference engine image Input Data(BGR)
+* @brief image input data for the inference engine supports
+*/
+typedef struct tagIEData {
+    void * buffer;
+    unsigned int size;  //byte size
+    unsigned int width;
+    unsigned int height;
+    unsigned int widthStride;
+    unsigned int heightStride;
+    unsigned int channelNum;
+    unsigned int batchIdx;
+    IEPrecisionType precision; //IE_FP32:IE_FP16:IE_U8
+    IEMemoryType memType;
+    IEImageFormatType imageFormat;
+    IEDataType dataType;
+}IEData;
+
+/**
+* @struct inference engine Context Configuration
+* @brief Configuration for the inference engine supports
+*/
+typedef struct tagIEConfig {
+    IETargetDeviceType targetId;
+    IEInputOutputInfo inputInfos;
+    IEInputOutputInfo outputInfos;
+
+    char * pluginPath;
+    char * cpuExtPath;
+    char * cldnnExtPath;
+    char * modelFileName; // Bin file name
+    char * outputLayerName;
+    unsigned int  perfCounter;
+    unsigned int inferReqNum;   // it work with async mode and value is 1 in default.
+}IEConfig;
+
+#ifdef __cplusplus
+}
+#endif
+#endif
diff --git a/inference-engine/src/CMakeLists.txt b/inference-engine/src/CMakeLists.txt
index cabd78b..9ddd736 100644
--- a/inference-engine/src/CMakeLists.txt
+++ b/inference-engine/src/CMakeLists.txt
@@ -23,6 +23,7 @@ if (ENABLE_GNA)
 endif()
 
 add_subdirectory(hetero_plugin)
+add_subdirectory(ie_c_wrapper)
 
 set(InferenceEngine_LIBRARIES inference_engine)
 set(InferenceEngine_INCLUDE_DIRS ${CMAKE_SOURCE_DIR}/include)
diff --git a/inference-engine/src/ie_c_wrapper/CMakeLists.txt b/inference-engine/src/ie_c_wrapper/CMakeLists.txt
new file mode 100644
index 0000000..063f923
--- /dev/null
+++ b/inference-engine/src/ie_c_wrapper/CMakeLists.txt
@@ -0,0 +1,29 @@
+# Copyright (C) 2018 Intel Corporation
+# SPDX-License-Identifier: Apache-2.0
+#
+
+set (TARGET_NAME "inference_engine_c_wrapper")
+
+file(GLOB SOURCES
+    ${CMAKE_CURRENT_SOURCE_DIR}/*.cpp
+)
+
+file(GLOB HEADERS
+    ${CMAKE_CURRENT_SOURCE_DIR}/*.h
+    ${CMAKE_CURRENT_SOURCE_DIR}/*.hpp
+)
+
+addVersionDefines(ie_api_wrapper.cpp CI_BUILD_NUMBER)
+
+include_directories(
+    ${IE_MAIN_SOURCE_DIR}/src/inference_engine
+    ${CMAKE_CURRENT_SOURCE_DIR}
+)
+
+if(WIN32)
+    add_definitions(-DIMPLEMENT_INFERENCE_ENGINE_API)
+endif()
+
+add_library(${TARGET_NAME} SHARED ${SOURCES} ${HEADERS})
+target_link_libraries(${TARGET_NAME} inference_engine IE::ie_cpu_extension)
+set_target_properties(${TARGET_NAME} PROPERTIES COMPILE_PDB_NAME ${TARGET_NAME})
diff --git a/inference-engine/src/ie_c_wrapper/README.md b/inference-engine/src/ie_c_wrapper/README.md
new file mode 100644
index 0000000..af0004c
--- /dev/null
+++ b/inference-engine/src/ie_c_wrapper/README.md
@@ -0,0 +1 @@
+this is the wrapper library for the inference engine library. 
diff --git a/inference-engine/src/ie_c_wrapper/ie_api_wrapper.cpp b/inference-engine/src/ie_c_wrapper/ie_api_wrapper.cpp
new file mode 100644
index 0000000..f8c9b3c
--- /dev/null
+++ b/inference-engine/src/ie_c_wrapper/ie_api_wrapper.cpp
@@ -0,0 +1,132 @@
+ // Licensed under the Apache License, Version 2.0 (the "License");
+ // you may not use this file except in compliance with the License.
+ // You may obtain a copy of the License at
+ //
+ //      http://www.apache.org/licenses/LICENSE-2.0
+ //
+ // Unless required by applicable law or agreed to in writing, software
+ // distributed under the License is distributed on an "AS IS" BASIS,
+ // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ // See the License for the specific language governing permissions and
+ // limitations under the License.
+
+#include "ie_api_wrapper.h"
+#include "ie_context.h"
+
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+using namespace std;
+using namespace InferenceEngine;
+
+int IESizeOfContext()
+{
+    return sizeof(InferenceEngine::CIEContext);
+}
+
+void * IEAllocateContext()
+{
+    InferenceEngine::CIEContext * context = new InferenceEngine::CIEContext();
+    return (reinterpret_cast<void *>(context));
+}
+
+void * IEAllocateContextWithConfig(IEConfig * config)
+{
+    InferenceEngine::CIEContext * context = new InferenceEngine::CIEContext(config);
+    return (reinterpret_cast<void *>(context));
+}
+
+void IEFreeContext(void * contextPtr)
+{
+    delete(reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr));
+}
+
+void IELoadModel(void * contextPtr, IEConfig * config)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+    context->loadModel(config);
+}
+
+void IECreateModel(void * contextPtr, IEConfig * config)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+    context->createModel(config);
+}
+
+void IEGetModelInputImageSize(void * contextPtr, IEImageSize * imageSize)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+    context->getModelInputSize(imageSize);
+}
+
+void IEGetModelInputInfo(void * contextPtr, IEInputOutputInfo * info)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+    context->getModelInputInfo(info);
+}
+
+void IESetModelInputInfo(void * contextPtr, IEInputOutputInfo * info)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+    context->setModelInputInfo(info);
+}
+
+void IEGetModelOutputInfo(void * contextPtr, IEInputOutputInfo * info)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+    context->getModelOutputInfo(info);
+}
+
+void IESetModelOutputInfo(void * contextPtr, IEInputOutputInfo * info)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+    context->setModelOutputInfo(info);
+}
+
+void IEForward(void * contextPtr, IEInferMode aSyncMode)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+
+    if (IE_INFER_MODE_SYNC == aSyncMode) {
+        context->forwardSync();
+    }
+    else {
+        context->forwardAsync();
+    }
+}
+
+void IESetInput(void * contextPtr, unsigned int idx, IEData * data)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+    context->addInput(idx, data);
+}
+
+void * IEGetResultSpace(void * contextPtr, unsigned int idx, unsigned int * size)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+    return context->getOutput(idx, size);
+}
+
+void IEPrintLog(void * contextPtr, unsigned int flag)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+    return context->printLog(flag);
+}
+
+void IESetBatchSize(void *contextPtr, int size)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+    return context->setBatchSize(size);
+}
+
+void IESetCpuThreadsNum(void *contextPtr, unsigned int num)
+{
+    InferenceEngine::CIEContext * context = reinterpret_cast<InferenceEngine::CIEContext *>(contextPtr);
+    return context->setCpuThreadsNum(num);
+}
+
+#ifdef __cplusplus
+}
+#endif
diff --git a/inference-engine/src/ie_c_wrapper/ie_context.cpp b/inference-engine/src/ie_c_wrapper/ie_context.cpp
new file mode 100644
index 0000000..11910ef
--- /dev/null
+++ b/inference-engine/src/ie_c_wrapper/ie_context.cpp
@@ -0,0 +1,834 @@
+// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/**
+ * @brief A header file that provides wrapper for IE context object
+ * @file ie_context.h
+ */
+#include "ie_context.h"
+
+using namespace std;
+
+namespace InferenceEngine {
+
+CIEContext::CIEContext()
+{
+    bModelLoaded = false;
+    bModelCreated = false;
+    targetDevice = InferenceEngine::TargetDevice::eCPU;
+    modelInputImageSize.imageHeight = 0;
+    modelInputImageSize.imageWidth = 0;
+}
+
+CIEContext::~CIEContext()
+{
+
+}
+
+CIEContext::CIEContext(IEConfig * config)
+{
+    bModelLoaded = false;
+    bModelCreated = false;
+    modelInputImageSize.imageHeight = 0;
+    modelInputImageSize.imageWidth = 0;
+    targetDevice = InferenceEngine::TargetDevice::eCPU;
+    Init(config);
+    bModelLoaded = true;
+    bModelCreated = true;
+}
+
+void CIEContext::loadModel(IEConfig * config)
+{
+    if (bModelLoaded) return;
+
+    std::string path("");
+    if (config->pluginPath)
+        path.assign(config->pluginPath);
+
+    InferenceEngine::PluginDispatcher dispatcher({ path, "../../../lib/intel64", "" });
+    targetDevice = getDeviceFromId(config->targetId);
+    /** Loading plugin for device **/
+    plugin = dispatcher.getPluginByDevice(getDeviceName(targetDevice));
+    enginePtr = plugin;
+    if (nullptr == enginePtr) {
+        std::cout << "Plugin path is not found!" << std::endl;
+        std::cout << "Plugin Path =" << path << std::endl;
+    }
+    std::cout << "targetDevice:" << getDeviceName(targetDevice) << std::endl;
+
+    /*If CPU device, load default library with extensions that comes with the product*/
+    if (config->targetId == IE_CPU) {
+        /**
+        * cpu_extensions library is compiled from "extension" folder containing
+        * custom MKLDNNPlugin layer implementations. These layers are not supported
+        * by mkldnn, but they can be useful for inferring custom topologies.
+        **/
+        plugin.AddExtension(std::make_shared<Extensions::Cpu::CpuExtensions>());
+    }
+
+    if (nullptr != config->cpuExtPath) {
+        std::string cpuExtPath(config->cpuExtPath);
+        // CPU(MKLDNN) extensions are loaded as a shared library and passed as a pointer to base extension
+        auto extensionPtr = InferenceEngine::make_so_pointer<InferenceEngine::IExtension>(cpuExtPath);
+        plugin.AddExtension(extensionPtr);
+        std::cout << "CPU Extension loaded: " << cpuExtPath << endl;
+    }
+    if (nullptr != config->cldnnExtPath) {
+        std::string cldnnExtPath(config->cldnnExtPath);
+        // clDNN Extensions are loaded from an .xml description and OpenCL kernel files
+        plugin.SetConfig({ { InferenceEngine::PluginConfigParams::KEY_CONFIG_FILE, cldnnExtPath } });
+        std::cout << "GPU Extension loaded: " << cldnnExtPath << endl;
+    }
+
+    /** Setting plugin parameter for collecting per layer metrics **/
+    if (config->perfCounter > 0) {
+        plugin.SetConfig({ { InferenceEngine::PluginConfigParams::KEY_PERF_COUNT, InferenceEngine::PluginConfigParams::YES } });
+    }
+
+    std::string modelFileName(config->modelFileName);
+    if (modelFileName.empty()) {
+        std::cout << "Model file name is empty!" << endl;
+        return;
+    }
+
+    xmlFile = GetFileNameNoExt(config->modelFileName) + ".xml";
+    std::string networkFileName(xmlFile);
+    networkReader.ReadNetwork(networkFileName);
+
+    binFile = GetFileNameNoExt(config->modelFileName) + ".bin";
+    std::string weightFileName(binFile);
+    networkReader.ReadWeights(weightFileName);
+
+    network = networkReader.getNetwork();
+    inputsInfo = network.getInputsInfo();
+    outputsInfo = network.getOutputsInfo();
+
+    bModelLoaded = true;
+}
+
+void CIEContext::createModel(IEConfig * config)
+{
+    if (!bModelLoaded) {
+        std::cout << "Please load the model firstly!" << endl;
+        return;
+    }
+
+    if (bModelCreated) return;
+
+    // prepare the input and output Blob
+    // Set the precision of intput/output data provided by the user, should be called before load of the network to the plugin
+
+    setModelInputInfo(&config->inputInfos);
+    setModelOutputInfo(&config->outputInfos);
+
+    executeNetwork = plugin.LoadNetwork(network, networkConfig);
+    inferRequest = executeNetwork.CreateInferRequest();
+    bModelCreated = true;
+}
+
+void CIEContext::Init(IEConfig * config)
+{
+    if (bModelLoaded && bModelCreated) return;
+
+    if (!bModelLoaded) {
+        loadModel(config);
+    }
+
+    if (!bModelCreated) {
+        createModel(config);
+    }
+}
+
+void CIEContext::setTargetDevice(InferenceEngine::TargetDevice device)
+{
+    targetDevice = device;
+}
+
+void CIEContext::setCpuThreadsNum(const size_t num)
+{
+    networkConfig[PluginConfigParams::KEY_CPU_THREADS_NUM] = std::to_string(num);
+}
+
+void CIEContext::setBatchSize(const size_t size)
+{
+    network.setBatchSize(size);
+}
+
+size_t CIEContext::getBatchSize()
+{
+    return network.getBatchSize();
+}
+
+void CIEContext::forwardSync()
+{
+    inferRequest.Infer();
+}
+
+void CIEContext::forwardAsync()
+{
+    inferRequest.StartAsync();
+    inferRequest.Wait(IInferRequest::WaitMode::RESULT_READY);
+}
+
+size_t CIEContext::getInputSize()
+{
+    return inputsInfo.size();
+}
+
+void CIEContext::setInputPresion(unsigned int idx, IEPrecisionType precision)
+{
+    unsigned int id = 0;
+
+    if (idx > inputsInfo.size()) {
+        std::cout << "Input is out of range" << std::endl;
+        return;
+    }
+    /** Iterating over all input blobs **/
+    for (auto & item : inputsInfo) {
+        if (id == idx) {
+            /** Creating first input blob **/
+            Precision inputPrecision = getPrecisionByEnum(precision);
+            item.second->setPrecision(inputPrecision);
+            break;
+        }
+        id++;
+    }
+}
+
+void CIEContext::setOutputPresion(unsigned int idx, IEPrecisionType precision)
+{
+    unsigned int id = 0;
+
+    if (idx > outputsInfo.size()) {
+        std::cout << "Output is out of range" << std::endl;
+        return;
+    }
+    /** Iterating over all output blobs **/
+    for (auto & item : outputsInfo) {
+        if (id == idx) {
+            /** Creating first input blob **/
+            Precision outputPrecision = getPrecisionByEnum(precision);
+            item.second->setPrecision(outputPrecision);
+            break;
+        }
+        id++;
+    }
+}
+
+void CIEContext::setInputLayout(unsigned int idx, IELayoutType layout)
+{
+    unsigned int id = 0;
+
+    if (idx > inputsInfo.size()) {
+        std::cout << "Input is out of range" << std::endl;
+        return;
+    }
+    /** Iterating over all input blobs **/
+    for (auto & item : inputsInfo) {
+        if (id == idx) {
+            /** Creating first input blob **/
+            Layout inputLayout = getLayoutByEnum(layout);
+            item.second->setLayout(inputLayout);
+            break;
+        }
+        id++;
+    }
+
+}
+
+void CIEContext::setOutputLayout(unsigned int idx, IELayoutType layout)
+{
+    unsigned int id = 0;
+
+    if (idx > outputsInfo.size()) {
+        std::cout << "Output is out of range" << std::endl;
+        return;
+    }
+    /** Iterating over all output blobs **/
+    for (auto & item : outputsInfo) {
+        if (id == idx) {
+            Layout outputLayout = getLayoutByEnum(layout);
+            item.second->setLayout(outputLayout);
+            break;
+        }
+        id++;
+    }
+
+}
+
+void CIEContext::getModelInputSize(IEImageSize * imageSize)
+{
+    if (!bModelCreated) {
+        std::cout << "Please create the model firstly!" << endl;
+        return;
+    }
+
+    imageSize->imageHeight = modelInputImageSize.imageHeight;
+    imageSize->imageWidth = modelInputImageSize.imageWidth;
+    return;
+}
+
+void CIEContext::getModelInputInfo(IEInputOutputInfo * info)
+{
+    if (!bModelLoaded) {
+        std::cout << "Please load the model firstly!" << endl;
+        return;
+    }
+
+    int id = 0;
+    for (auto & item : inputsInfo) {
+        info->width[id] = item.second->getDims()[0];
+        info->height[id] = item.second->getDims()[1];
+        info->channels[id] = item.second->getDims()[2];
+        info->precision[id] = getEnumByPrecision(item.second->getPrecision());
+        info->layout[id] = getEnumByLayout(item.second->getLayout());
+        id++;
+    }
+    info->batch_size = getBatchSize();
+    info->numbers = inputsInfo.size();
+}
+
+void CIEContext::setModelInputInfo(IEInputOutputInfo * info)
+{
+    int id = 0;
+
+    if (!bModelLoaded) {
+        std::cout << "Please load the model firstly!" << endl;
+        return;
+    }
+
+    if (info->numbers != inputsInfo.size()) {
+        std::cout << "Input size is not matched with model!" << endl;
+        return;
+    }
+
+    for (auto & item : inputsInfo) {
+        Precision precision = getPrecisionByEnum(info->precision[id]);
+        item.second->setPrecision(precision);
+        Layout layout = getLayoutByEnum(info->layout[id]);
+        item.second->setLayout(layout);
+
+        if (info->dataType[id] == IE_DATA_TYPE_IMG) {
+            modelInputImageSize.imageHeight = inputsInfo[item.first]->getDims()[1];
+            modelInputImageSize.imageWidth = inputsInfo[item.first]->getDims()[0];
+        }
+
+        id++;
+    }
+}
+
+void CIEContext::getModelOutputInfo(IEInputOutputInfo * info)
+{
+    int id = 0;
+
+    if (!bModelLoaded) {
+        std::cout << "Please load the model firstly!" << endl;
+        return;
+    }
+
+    for (auto & item : outputsInfo) {
+        auto& _output = item.second;
+        const InferenceEngine::SizeVector outputDims = _output->dims;
+        info->width[id]    = outputDims[0];
+        info->height[id]   = outputDims[1];
+        info->channels[id] = outputDims[2];
+        info->precision[id] = getEnumByPrecision(_output->getPrecision());
+        info->layout[id] = getEnumByLayout(_output->getLayout());
+        id++;
+    }
+    info->batch_size = 0;
+    info->numbers = outputsInfo.size();
+}
+
+void CIEContext::setModelOutputInfo(IEInputOutputInfo * info)
+{
+    int id = 0;
+
+    if (!bModelLoaded) {
+        std::cout << "Please load the model firstly!" << endl;
+        return;
+    }
+
+    if (info->numbers != outputsInfo.size()) {
+        std::cout << "Output size is not matched with model!" << endl;
+        return;
+    }
+
+    for (auto & item : outputsInfo) {
+        Precision precision = getPrecisionByEnum(info->precision[id]);
+        item.second->setPrecision(precision);
+        Layout layout = getLayoutByEnum(info->layout[id]);
+        item.second->setLayout(layout);
+        id++;
+    }
+}
+
+void CIEContext::addInput(unsigned int idx, IEData * data)
+{
+    unsigned int id = 0;
+    std::string itemName;
+
+    if (idx > inputsInfo.size()) {
+        std::cout << "Input is out of range" << std::endl;
+        return;
+    }
+    /** Iterating over all input blobs **/
+    for (auto & item : inputsInfo) {
+        if (id == idx) {
+            itemName = item.first;
+            break;
+        }
+        id++;
+    }
+
+    if (itemName.empty()) {
+        std::cout << "item name is empty!" << std::endl;
+        return;
+    }
+
+    if (data->batchIdx > getBatchSize()) {
+        std::cout << "Too many input, it is bigger than batch size!" << std::endl;
+        return;
+    }
+
+    Blob::Ptr blob = inferRequest.GetBlob(itemName);
+    if (data->precision == IE_FP32) {
+        if(data->dataType == IE_DATA_TYPE_IMG)
+            imageU8ToBlob<PrecisionTrait<Precision::FP32>::value_type>(data, blob, data->batchIdx);
+        else
+            nonImageToBlob<PrecisionTrait<Precision::FP32>::value_type>(data, blob, data->batchIdx);
+    } else {
+        if(data->dataType == IE_DATA_TYPE_IMG)
+            imageU8ToBlob<uint8_t>(data, blob, data->batchIdx);
+        else
+            nonImageToBlob<uint8_t>(data, blob, data->batchIdx);
+    }
+}
+
+void * CIEContext::getOutput(unsigned int idx, unsigned int * size)
+{
+    unsigned int id = 0;
+    std::string itemName;
+
+    if (idx > outputsInfo.size()) {
+        std::cout << "Output is out of range" << std::endl;
+        return nullptr;
+    }
+    /** Iterating over all input blobs **/
+    for (auto & item : outputsInfo) {
+        if (id == idx) {
+            itemName = item.first;
+            break;
+        }
+        id++;
+    }
+
+    if (itemName.empty()) {
+        std::cout << "item name is empty!" << std::endl;
+        return nullptr;
+    }
+
+    const Blob::Ptr blob = inferRequest.GetBlob(itemName);
+    float* outputResult = static_cast<PrecisionTrait<Precision::FP32>::value_type*>(blob->buffer());
+
+    *size = blob->byteSize();
+    return (reinterpret_cast<void *>(outputResult));
+}
+
+InferenceEngine::TargetDevice CIEContext::getDeviceFromString(const std::string &deviceName) {
+    return InferenceEngine::TargetDeviceInfo::fromStr(deviceName);
+}
+
+InferenceEngine::TargetDevice CIEContext::getDeviceFromId(IETargetDeviceType device) {
+    switch (device) {
+    case IE_Default:
+        return InferenceEngine::TargetDevice::eDefault;
+    case IE_Balanced:
+        return InferenceEngine::TargetDevice::eBalanced;
+    case IE_CPU:
+        return InferenceEngine::TargetDevice::eCPU;
+    case IE_GPU:
+        return InferenceEngine::TargetDevice::eGPU;
+    case IE_FPGA:
+        return InferenceEngine::TargetDevice::eFPGA;
+    case IE_MYRIAD:
+        return InferenceEngine::TargetDevice::eMYRIAD;
+    case IE_HDDL:
+        return InferenceEngine::TargetDevice::eHDDL;
+    case IE_GNA:
+        return InferenceEngine::TargetDevice::eGNA;
+    case IE_HETERO:
+        return InferenceEngine::TargetDevice::eHETERO;
+    default:
+        return InferenceEngine::TargetDevice::eCPU;
+    }
+}
+
+InferenceEngine::Layout CIEContext::estimateLayout(const int chNum)
+{
+    if (chNum == 4)
+        return InferenceEngine::Layout::NCHW;
+    else if (chNum == 2)
+        return InferenceEngine::Layout::NC;
+    else if (chNum == 3)
+        return InferenceEngine::Layout::CHW;
+    else
+        return InferenceEngine::Layout::ANY;
+}
+
+InferenceEngine::Layout CIEContext::getLayoutByEnum(IELayoutType layout)
+{
+    switch (layout) {
+    case IE_NCHW:
+        return InferenceEngine::Layout::NCHW;
+    case IE_NHWC:
+        return InferenceEngine::Layout::NHWC;
+    case IE_OIHW:
+        return InferenceEngine::Layout::OIHW;
+    case IE_C:
+        return InferenceEngine::Layout::C;
+    case IE_CHW:
+        return InferenceEngine::Layout::CHW;
+    case IE_HW:
+        return InferenceEngine::Layout::HW;
+    case IE_NC:
+        return InferenceEngine::Layout::NC;
+    case IE_CN:
+        return InferenceEngine::Layout::CN;
+    case IE_BLOCKED:
+        return InferenceEngine::Layout::BLOCKED;
+    case IE_ANY:
+        return InferenceEngine::Layout::ANY;
+    default:
+        return InferenceEngine::Layout::ANY;
+    }
+
+}
+
+IELayoutType CIEContext::getEnumByLayout(InferenceEngine::Layout layout)
+{
+    switch (layout) {
+    case InferenceEngine::Layout::NCHW:
+        return IE_NCHW;
+    case InferenceEngine::Layout::NHWC:
+        return IE_NHWC;
+    case InferenceEngine::Layout::OIHW:
+        return IE_OIHW;
+    case InferenceEngine::Layout::C:
+        return IE_C;
+    case InferenceEngine::Layout::CHW:
+        return IE_CHW;
+    case InferenceEngine::Layout::HW:
+        return IE_HW;
+    case InferenceEngine::Layout::NC:
+        return IE_NC;
+    case InferenceEngine::Layout::CN:
+        return IE_CN;
+    case InferenceEngine::Layout::BLOCKED:
+        return IE_BLOCKED;
+    case InferenceEngine::Layout::ANY:
+        return IE_ANY;
+    default:
+        return IE_ANY;
+    }
+}
+
+InferenceEngine::Precision CIEContext::getPrecisionByEnum(IEPrecisionType precision)
+{
+    switch (precision) {
+    case IE_MIXED:
+        return InferenceEngine::Precision::MIXED;
+    case IE_FP32:
+        return InferenceEngine::Precision::FP32;
+    case IE_FP16:
+        return InferenceEngine::Precision::FP16;
+    case IE_Q78:
+        return InferenceEngine::Precision::Q78;
+    case IE_I16:
+        return InferenceEngine::Precision::I16;
+    case IE_U8:
+        return InferenceEngine::Precision::U8;
+    case IE_I8:
+        return InferenceEngine::Precision::I8;
+    case IE_U16:
+        return InferenceEngine::Precision::U16;
+    case IE_I32:
+        return InferenceEngine::Precision::I32;
+    case IE_CUSTOM:
+        return InferenceEngine::Precision::CUSTOM;
+    case IE_UNSPECIFIED:
+        return InferenceEngine::Precision::UNSPECIFIED;
+    default:
+        return InferenceEngine::Precision::UNSPECIFIED;
+    }
+}
+
+IEPrecisionType CIEContext::getEnumByPrecision(InferenceEngine::Precision precision)
+{
+    switch (precision) {
+    case InferenceEngine::Precision::MIXED:
+        return IE_MIXED;
+    case InferenceEngine::Precision::FP32:
+        return IE_FP32;
+    case InferenceEngine::Precision::FP16:
+        return IE_FP16;
+    case InferenceEngine::Precision::Q78:
+        return IE_Q78;
+    case InferenceEngine::Precision::I16:
+        return IE_I16;
+    case InferenceEngine::Precision::U8:
+        return IE_U8;
+    case InferenceEngine::Precision::I8:
+        return IE_I8;
+    case InferenceEngine::Precision::U16:
+        return IE_U16;
+    case InferenceEngine::Precision::I32:
+        return IE_I32;
+    case InferenceEngine::Precision::CUSTOM:
+        return IE_CUSTOM;
+    case InferenceEngine::Precision::UNSPECIFIED:
+        return IE_UNSPECIFIED;
+    default:
+        return IE_UNSPECIFIED;
+    }
+}
+
+std::string CIEContext::GetFileNameNoExt(const std::string &filePath) {
+    auto pos = filePath.rfind('.');
+    if (pos == std::string::npos) return filePath;
+    return filePath.substr(0, pos);
+}
+
+void CIEContext::getTopNResultWithClassfication(unsigned int idx, int topN)
+{
+
+    size_t batchSize = getBatchSize();
+    unsigned int id = 0;
+    std::string itemName;
+
+    if (idx > outputsInfo.size()) {
+        std::cout << "Output is out of range" << std::endl;
+        return;
+    }
+    /** Iterating over all input blobs **/
+    for (auto & item : outputsInfo) {
+        if (id == idx) {
+            itemName = item.first;
+            break;
+        }
+        id++;
+    }
+
+    if (itemName.empty()) {
+        std::cout << "item name is empty!" << std::endl;
+        return;
+    }
+
+    /** Validating -nt value **/
+    const auto outputBlob = inferRequest.GetBlob(itemName);
+    const auto outputData = outputBlob->buffer().as<PrecisionTrait<Precision::FP32>::value_type*>();
+    const int resultsCnt = outputBlob->size() / batchSize;
+
+    if (topN > resultsCnt || topN < 1) {
+        std::cout << "TopN " << topN << " is not available for this network (TopN should be less than " \
+            << resultsCnt + 1 << " and more than 0)\n            will be used maximal value : " << resultsCnt <<std::endl;
+        topN = resultsCnt;
+    }
+
+    /** This vector stores id's of top N results **/
+    std::vector<unsigned> results;
+    InferenceEngine::TopResults(topN, *outputBlob, results);
+
+    std::cout << std::endl << "Top " << topN << " results:" << std::endl << std::endl;
+
+    /** Read labels from file (e.x. AlexNet.labels) **/
+    bool labelsEnabled = false;
+    std::string labelFileName = GetFileNameNoExt(xmlFile) + ".labels";
+    std::vector<std::string> labels;
+
+    std::ifstream inputFile;
+    inputFile.open(labelFileName, std::ios::in);
+    if (inputFile.is_open()) {
+        std::string strLine;
+        while (std::getline(inputFile, strLine)) {
+            labels.push_back(strLine);
+        }
+        labelsEnabled = true;
+    }
+
+    /** Print the result iterating over each batch **/
+    for (int image_id = 0; image_id < batchSize; ++image_id) {
+        std::cout << "Image " << image_id << std::endl << std::endl;
+        for (size_t id = image_id * topN, cnt = 0; cnt < topN; ++cnt, ++id) {
+            std::cout.precision(7);
+            /** Getting probability for resulting class **/
+            const auto result = outputData[results[id] + image_id * (outputBlob->size() / batchSize)];
+            std::cout << std::left << std::fixed << results[id] << " " << result;
+            if (labelsEnabled) {
+                std::cout << " label " << labels[results[id]] << std::endl;
+            }
+            else {
+                std::cout << " label #" << results[id] << std::endl;
+            }
+        }
+        std::cout << std::endl;
+    }
+}
+
+template <typename T> void CIEContext::imageU8ToBlob(const IEData * data, Blob::Ptr& blob, int batchIndex)
+{
+    SizeVector blobSize = blob.get()->dims();
+    const size_t width = blobSize[0];
+    const size_t height = blobSize[1];
+    const size_t channels = blobSize[2];
+    const size_t imageSize = width * height;
+    unsigned char * buffer = (unsigned char *)data->buffer;
+    T* blob_data = blob->buffer().as<T*>();
+    const float mean_val = 127.5f;
+    const float std_val = 0.0078125f;
+
+    if (width != data->width || height!= data->height) {
+        std::cout << "Input Image size is not matched with model!" << endl;
+        return;
+    }
+
+    int batchOffset = batchIndex * height* width * channels;
+
+    if (IE_IMAGE_BGR_PLANAR == data->imageFormat) {
+        // B G R planar input image
+        /** Filling input tensor with images. First b channel, then g and r channels **/
+        size_t imageStrideSize = data->heightStride * data->widthStride;
+
+        if (data->width == data->widthStride &&
+            data->height == data->heightStride) {
+            std::memcpy(blob_data + batchOffset, buffer, imageSize * channels);
+        }
+        else if (data->width == data->widthStride) {
+            for (size_t ch = 0; ch < channels; ++ch) {
+                std::memcpy(blob_data + batchOffset + ch * imageSize, buffer + ch * imageStrideSize, imageSize);
+            }
+        }
+        else {
+            /** Iterate over all pixel in image (b,g,r) **/
+            for (size_t ch = 0; ch < channels; ++ch) {
+                for (size_t h = 0; h < height; h++) {
+                    std::memcpy(blob_data + batchOffset + ch * imageSize + h * width, buffer + ch * imageStrideSize + h * data->widthStride, width);
+                }
+            }
+        }
+    }
+    else if (IE_IMAGE_BGR_PACKED == data->imageFormat) {
+        // B G R packed input image
+        size_t imageStrideSize = data->channelNum * data->widthStride;
+
+        if (data->precision == IE_FP32) {
+            /** Iterate over all pixel in image (b,g,r) **/
+            for (size_t h = 0; h < height; h++)
+                for (size_t w = 0; w < width; w++)
+                    /** Iterate over all channels **/
+                    for (size_t ch = 0; ch < channels; ++ch)
+                        blob_data[batchOffset + ch * imageSize + h * width + w] = float((buffer[h * imageStrideSize + w * data->channelNum + ch] - mean_val) * std_val);
+        } else {
+            /** Iterate over all pixel in image (b,g,r) **/
+            for (size_t h = 0; h < height; h++)
+                for (size_t w = 0; w < width; w++)
+                    /** Iterate over all channels **/
+                    for (size_t ch = 0; ch < channels; ++ch)
+                        blob_data[batchOffset + ch * imageSize + h * width + w] = buffer[h * imageStrideSize + w * data->channelNum + ch];
+        }
+    }
+    else if (IE_IMAGE_RGB_PLANAR == data->imageFormat) {
+        // R G B planar input image, switch the R and B plane. TBD
+
+    }
+    else if (IE_IMAGE_RGB_PACKED == data->imageFormat) {
+        // R G B packed input image, wwitch the R and B packed value. TBD
+
+    }
+    else if (IE_IMAGE_GRAY_PLANAR == data->imageFormat) {
+
+    }
+}
+
+template <typename T> void CIEContext::nonImageToBlob(const IEData * data, Blob::Ptr& blob, int batchIndex)
+{
+    SizeVector blobSize = blob.get()->dims();
+    T * buffer = (T *)data->buffer;
+    T* blob_data = blob->buffer().as<T*>();
+
+    int batchOffset = batchIndex * data->size;
+
+    memcpy(blob_data+batchOffset, buffer, data->size);
+}
+
+void CIEContext::printPerformanceCounts(const std::map<std::string, InferenceEngine::InferenceEngineProfileInfo>& performanceMap, std::ostream &stream, bool bshowHeader)
+{
+    long long totalTime = 0;
+    // Print performance counts
+    if (bshowHeader) {
+        stream << std::endl << "performance counts:" << std::endl << std::endl;
+    }
+    for (const auto & it : performanceMap) {
+        std::string toPrint(it.first);
+        const int maxLayerName = 30;
+
+        if (it.first.length() >= maxLayerName) {
+            toPrint = it.first.substr(0, maxLayerName - 4);
+            toPrint += "...";
+        }
+
+
+        stream << std::setw(maxLayerName) << std::left << toPrint;
+        switch (it.second.status) {
+        case InferenceEngine::InferenceEngineProfileInfo::EXECUTED:
+            stream << std::setw(15) << std::left << "EXECUTED";
+            break;
+        case InferenceEngine::InferenceEngineProfileInfo::NOT_RUN:
+            stream << std::setw(15) << std::left << "NOT_RUN";
+            break;
+        case InferenceEngine::InferenceEngineProfileInfo::OPTIMIZED_OUT:
+            stream << std::setw(15) << std::left << "OPTIMIZED_OUT";
+            break;
+        }
+        stream << std::setw(30) << std::left << "layerType: " + std::string(it.second.layer_type) + " ";
+        stream << std::setw(20) << std::left << "realTime: " + std::to_string(it.second.realTime_uSec);
+        stream << std::setw(20) << std::left << " cpu: " + std::to_string(it.second.cpu_uSec);
+        stream << " execType: " << it.second.exec_type << std::endl;
+        if (it.second.realTime_uSec > 0) {
+            totalTime += it.second.realTime_uSec;
+        }
+    }
+    stream << std::setw(20) << std::left << "Total time: " + std::to_string(totalTime) << " microseconds" << std::endl;
+}
+
+void CIEContext::printLog(unsigned int flag)
+{
+    std::map<std::string, InferenceEngine::InferenceEngineProfileInfo> perfomanceMap;
+
+    if (flag == IE_LOG_LEVEL_NONE)
+        return;
+
+    if (flag&IE_LOG_LEVEL_ENGINE) {
+        enginePtr->GetPerformanceCounts(perfomanceMap, nullptr);
+        printPerformanceCounts(perfomanceMap, std::cout, true);
+    }
+
+    if (flag&IE_LOG_LEVEL_LAYER) {
+        perfomanceMap = inferRequest.GetPerformanceCounts();
+        printPerformanceCounts(perfomanceMap, std::cout, true);
+    }
+}
+
+}  // namespace InferenceEngine
+
diff --git a/inference-engine/src/ie_c_wrapper/ie_context.h b/inference-engine/src/ie_c_wrapper/ie_context.h
new file mode 100644
index 0000000..61c866b
--- /dev/null
+++ b/inference-engine/src/ie_c_wrapper/ie_context.h
@@ -0,0 +1,265 @@
+// Copyright (c) 2018 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//      http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+/**
+ * @brief A header file that provides wrapper for IE context object
+ * @file ie_context.h
+ */
+
+#ifndef _IE_CONTEXT_H_
+#define _IE_CONTEXT_H_
+
+#pragma once
+
+
+#include <vector>
+#include <string>
+#include <fstream>
+#include <map>
+#include <utility>
+#include <memory>
+#include <iomanip>
+#include <inference_engine.hpp>
+#include <ext_list.hpp>
+#include "ie_common_wrapper.h"
+
+namespace InferenceEngine {
+
+/**
+ * @brief This class contains all the information about the Inference Engine Context
+ */
+class CIEContext {
+public:
+    /**
+    * @brief A default constructor
+    */
+    CIEContext();
+
+    /**
+    * @brief A default constructor
+    */
+    ~CIEContext();
+
+    /**
+    * @brief A constructor with IE configuration
+    */
+    CIEContext(IEConfig * config);
+
+    /**
+    * @brief load the model with graph and weight Model IR file
+    */
+    void loadModel(IEConfig * config);
+
+    /**
+    * @brief create the model/network with Model IR
+    */
+    void createModel(IEConfig * config);
+
+    /**
+    * @brief A init function withou model load
+    */
+    void Init(IEConfig * config);
+
+    /**
+    * @brief set target device
+    */
+    void setTargetDevice(InferenceEngine::TargetDevice device);
+
+    void setCpuThreadsNum(const size_t num);
+
+    /**
+    * @brief set batch size device
+    */
+    void setBatchSize(const size_t size);
+
+    /**
+    * @brief set batch size device
+    */
+    size_t getBatchSize();
+
+    /**
+    * @brief check initialized
+    */
+    bool isContextInitailized() {
+        return (bModelLoaded && bModelCreated);
+    };
+
+    /**
+    * @brief inference on the input data
+    */
+    void forwardSync();
+
+    /**
+    * @brief async inference on the input data
+    */
+    void forwardAsync();
+
+    /**
+    * @brief get input size
+    */
+    size_t getInputSize();
+
+    /**
+    * @brief set input precision
+    */
+    void setInputPresion(unsigned int idx, IEPrecisionType precision);
+
+    /**
+    * @brief set output precision
+    */
+    void setOutputPresion(unsigned int idx, IEPrecisionType precision);
+
+    /**
+    * @brief set input precision
+    */
+    void setInputLayout(unsigned int idx, IELayoutType layout);
+
+    /**
+    * @brief set output precision
+    */
+    void setOutputLayout(unsigned int idx, IELayoutType layout);
+
+    /**
+    * @brief get model input image size
+    */
+    void getModelInputSize(IEImageSize * imageSize);
+
+    /**
+    * @brief get model input info
+    */
+    void getModelInputInfo(IEInputOutputInfo * info);
+
+    /**
+    * @brief set model input info
+    */
+    void setModelInputInfo(IEInputOutputInfo * info);
+
+    /**
+    * @brief get model output info
+    */
+    void getModelOutputInfo(IEInputOutputInfo * info);
+
+    /**
+    * @brief set model output info
+    */
+    void setModelOutputInfo(IEInputOutputInfo * info);
+
+    /**
+    * @brief add video input
+    */
+    void addInput(unsigned int idx, IEData * data);
+
+    /**
+    * @brief get output result
+    */
+    void * getOutput(unsigned int idx, unsigned int * size);
+
+    /**
+    * @brief Print the performance counter info
+    */
+    void printLog(unsigned int flag);
+
+protected:
+
+    /*
+    * @brief Converts string to TargetDevice
+    */
+    InferenceEngine::TargetDevice getDeviceFromString(const std::string &deviceName);
+
+    /*
+    * @brief Converts enum value to TargetDevice
+    */
+    InferenceEngine::TargetDevice getDeviceFromId(IETargetDeviceType device);
+
+    /*
+    * @brief estimate the data layout according to the channel number
+    */
+    InferenceEngine::Layout estimateLayout(const int chNum);
+
+    /*
+    * @brief Converts enum value to Layout
+    */
+    InferenceEngine::Layout getLayoutByEnum(IELayoutType layout);
+
+    /*
+    * @brief Converts lauout to enum value
+    */
+    IELayoutType getEnumByLayout(InferenceEngine::Layout layout);
+
+    /*
+    * @brief Converts enum value to Precision
+    */
+    InferenceEngine::Precision getPrecisionByEnum(IEPrecisionType precision);
+
+    /*
+    * @brief Converts precision to enum value
+    */
+    IEPrecisionType getEnumByPrecision(InferenceEngine::Precision precision);
+
+    /**
+    * @brief remove the file extension and keep the whole path
+    */
+    std::string GetFileNameNoExt(const std::string &filePath);
+
+    /**
+    * @brief get top N result
+    */
+    void getTopNResultWithClassfication(unsigned int idx, int topN);
+
+    /**
+    * @brief Convert image U8 to blob
+    */
+    template <typename T> void imageU8ToBlob(const IEData * data, Blob::Ptr& blob, int batchIndex);
+
+    /**
+    * @brief Convert Non image to blob
+    */
+    template <typename T> void nonImageToBlob(const IEData * data, Blob::Ptr& blob, int batchIndex);
+
+    /**
+    * @brief Print the performance counter info
+    */
+    void printPerformanceCounts(const std::map<std::string, InferenceEngine::InferenceEngineProfileInfo>& performanceMap, std::ostream &stream, bool bshowHeader);
+
+protected:
+
+    InferenceEngine::InferenceEnginePluginPtr enginePtr;
+    InferenceEngine::InferencePlugin plugin;
+    InferenceEngine::CNNNetReader networkReader;
+    InferenceEngine::CNNNetwork network;
+    InferenceEngine::ExecutableNetwork executeNetwork;
+    InferenceEngine::InferRequest inferRequest;
+
+    InferenceEngine::TargetDevice targetDevice;
+
+    InferenceEngine::InputsDataMap inputsInfo;
+    InferenceEngine::OutputsDataMap outputsInfo;
+
+    std::string xmlFile;
+    std::string binFile;
+
+    IEImageSize modelInputImageSize;
+
+    std::vector<InferenceEngine::CNNLayerPtr> layers;
+
+    std::map<std::string, std::string> networkConfig;
+
+    bool bModelLoaded;
+    bool bModelCreated;
+
+};
+
+}  // namespace InferenceEngine
+
+#endif
-- 
2.7.4

